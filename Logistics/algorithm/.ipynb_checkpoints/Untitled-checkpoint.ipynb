{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "from collections import deque\n",
    "from mlagents.envs import UnityEnvironment\n",
    "\n",
    "# DDPG를 위한 파라미터 값 세팅\n",
    "state_size = 29\n",
    "action_size = 3\n",
    "\n",
    "load_model = True\n",
    "train_mode = True\n",
    "\n",
    "batch_size = 128\n",
    "mem_maxlen = 50000\n",
    "discount_factor = 0.99\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.0001\n",
    "tau = 0.001\n",
    "\n",
    "mu = 0\n",
    "theta = 1e-3\n",
    "sigma = 2e-3\n",
    "\n",
    "start_train_episode = 100\n",
    "run_episode = 50000\n",
    "test_episode = 1000\n",
    "\n",
    "print_interval = 1\n",
    "save_interval = 5000\n",
    "\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H-%M-%S\")\n",
    "\n",
    "game = \"Logistics\"\n",
    "env_name = \"../Build/Drone\"\n",
    "\n",
    "save_path = \"../saved_models/Logistics/\" + date_time + \"_DDPG\"\n",
    "load_path = \"../saved_models/Logistics/20210703-21-21-48_DDPG/model/model5000\"\n",
    "\n",
    "# OU_noise 클래스 -> ou noise 정의 및 파라미터 결정\n",
    "class OU_noise:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.X = np.ones(action_size) * mu\n",
    "\n",
    "    def sample(self):\n",
    "        dx = theta * (mu - self.X) + sigma * np.random.randn(len(self.X))\n",
    "        self.X += dx\n",
    "        return self.X\n",
    "\n",
    "# Actor 클래스 -> Actor 클래스를 통해 action을 출력\n",
    "class Actor:\n",
    "    def __init__(self, name):\n",
    "        with tf.variable_scope(name):\n",
    "            self.state = tf.placeholder(tf.float32, [None, state_size])\n",
    "            self.fc1 = tf.layers.dense(self.state, 128, activation=tf.nn.relu)\n",
    "            self.fc2 = tf.layers.dense(self.fc1, 128, activation=tf.nn.relu)\n",
    "            self.action = tf.layers.dense(self.fc2, action_size, activation=tf.nn.tanh)\n",
    "\n",
    "        self.trainable_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, name)\n",
    "\n",
    "# Critic 클래스 -> Critic 클래스를 통해 state와 action에 대한 Q-value를 출력\n",
    "class Critic:\n",
    "    def __init__(self, name):\n",
    "        with tf.variable_scope(name):\n",
    "            self.state = tf.placeholder(tf.float32, [None, state_size])\n",
    "            self.fc1 = tf.layers.dense(self.state, 128, activation=tf.nn.relu)\n",
    "            self.action = tf.placeholder(tf.float32, [None, action_size])\n",
    "            self.concat = tf.concat([self.fc1, self.action],axis=-1)\n",
    "            self.fc2 = tf.layers.dense(self.concat, 128, activation=tf.nn.relu)\n",
    "            self.fc3 = tf.layers.dense(self.fc2, 128, activation=tf.nn.relu)\n",
    "            self.predict_q = tf.layers.dense(self.fc3, 1, activation=None)\n",
    "\n",
    "        self.trainable_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, name)\n",
    "    \n",
    "# DDPGAgnet 클래스 -> Actor-Critic을 기반으로 학습하는 에이전트 클래스\n",
    "class DDPGAgent:\n",
    "    def __init__(self, name):\n",
    "        self.model_name = name\n",
    "        self.actor = Actor(\"actor\" + name)\n",
    "        self.critic = Critic(\"critic\" + name)\n",
    "        self.target_actor = Actor(\"target_actor\" + name)\n",
    "        self.target_critic = Critic(\"target_critic\" + name)\n",
    "        \n",
    "        self.target_q = tf.placeholder(tf.float32, [None, 1])\n",
    "        critic_loss = tf.losses.mean_squared_error(self.target_q, self.critic.predict_q)\n",
    "        self.train_critic = tf.train.AdamOptimizer(critic_lr).minimize(critic_loss)\n",
    "\n",
    "        action_grad = tf.gradients(tf.squeeze(self.critic.predict_q), self.critic.action)\n",
    "        policy_grad = tf.gradients(self.actor.action, self.actor.trainable_var, action_grad)\n",
    "        for idx, grads in enumerate(policy_grad):\n",
    "            policy_grad[idx] = -grads/batch_size\n",
    "        self.train_actor = tf.train.AdamOptimizer(actor_lr).apply_gradients(\n",
    "                                                            zip(policy_grad, self.actor.trainable_var))\n",
    "  \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.Saver = tf.train.Saver()\n",
    "        self.Summary, self.Merge = self.Make_Summary()\n",
    "        self.OU = OU_noise()\n",
    "        self.memory = deque(maxlen=mem_maxlen)\n",
    "\n",
    "        self.soft_update_target = []\n",
    "        for idx in range(len(self.actor.trainable_var)):\n",
    "            self.soft_update_target.append(self.target_actor.trainable_var[idx].assign(\n",
    "                ((1 - tau) * self.target_actor.trainable_var[idx].value())\n",
    "                             + (tau * self.actor.trainable_var[idx].value())))\n",
    "        for idx in range(len(self.critic.trainable_var)):\n",
    "            self.soft_update_target.append(self.target_critic.trainable_var[idx].assign(\n",
    "                ((1 - tau) * self.target_critic.trainable_var[idx].value())\n",
    "                            + (tau * self.critic.trainable_var[idx].value())))\n",
    "        \n",
    "        init_update_target = []\n",
    "        for idx in range(len(self.actor.trainable_var)):\n",
    "            init_update_target.append(self.target_actor.trainable_var[idx].assign(\n",
    "                                      self.actor.trainable_var[idx]))\n",
    "        for idx in range(len(self.critic.trainable_var)):\n",
    "            init_update_target.append(self.target_critic.trainable_var[idx].assign(\n",
    "                                      self.critic.trainable_var[idx]))\n",
    "        self.sess.run(init_update_target)\n",
    "\n",
    "        if load_model == True:\n",
    "            self.Saver.restore(self.sess, load_path)\n",
    "\n",
    "    # Actor model에서 action을 예측하고 noise 설정\n",
    "    def get_action(self, state):\n",
    "        action = self.sess.run(self.actor.action, feed_dict={self.actor.state: state})\n",
    "        noise = self.OU.sample()\n",
    "        return action + noise if train_mode else action\n",
    "\n",
    "    # replay memory에 입력\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # model 저장\n",
    "    def save_model(self, episode):\n",
    "        self.Saver.save(self.sess, save_path + \"/model\" + self.model_name + \"/model_\" + episode)\n",
    "    \n",
    "    # replay memory를 통해 모델을 학습\n",
    "    def train_model(self):\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        states = np.asarray([sample[0] for sample in mini_batch])\n",
    "        actions = np.asarray([sample[1] for sample in mini_batch])\n",
    "        rewards = np.asarray([sample[2] for sample in mini_batch])\n",
    "        next_states = np.asarray([sample[3] for sample in mini_batch])\n",
    "        dones = np.asarray([sample[4] for sample in mini_batch])\n",
    "\n",
    "        target_actor_actions = self.sess.run(self.target_actor.action,\n",
    "                                            feed_dict={self.target_actor.state: next_states})\n",
    "        target_critic_predict_qs = self.sess.run(self.target_critic.predict_q,\n",
    "                                                feed_dict={self.target_critic.state: next_states,\n",
    "                                                self.target_critic.action: target_actor_actions})\n",
    "        target_qs = np.asarray([reward + discount_factor * (1 - done) * target_critic_predict_q\n",
    "                                for reward, target_critic_predict_q, done in zip(\n",
    "                                                        rewards, target_critic_predict_qs, dones)])\n",
    "        self.sess.run(self.train_critic, feed_dict={self.critic.state: states,\n",
    "                                                    self.critic.action: actions,\n",
    "                                                    self.target_q: target_qs})\n",
    "\n",
    "        actions_for_train = self.sess.run(self.actor.action, feed_dict={self.actor.state: states})\n",
    "        self.sess.run(self.train_actor, feed_dict={self.actor.state: states,\n",
    "                                                   self.critic.state: states,\n",
    "                                                   self.critic.action: actions_for_train})\n",
    "                                                   \n",
    "        self.sess.run(self.soft_update_target)\n",
    "\n",
    "    def Make_Summary(self):\n",
    "        self.summary_reward1 = tf.placeholder(tf.float32)\n",
    "        self.summary_reward2 = tf.placeholder(tf.float32)\n",
    "        self.summary_reward3 = tf.placeholder(tf.float32)\n",
    "        tf.summary.scalar(\"reward1\", self.summary_reward1)\n",
    "        tf.summary.scalar(\"reward2\", self.summary_reward2)\n",
    "        tf.summary.scalar(\"reward3\", self.summary_reward3)\n",
    "        Summary = tf.summary.FileWriter(logdir=save_path, graph=self.sess.graph)\n",
    "        Merge = tf.summary.merge_all()\n",
    "\n",
    "        return Summary, Merge\n",
    "        \n",
    "    def Write_Summray(self, r1, r2, r3, episode):\n",
    "        self.Summary.add_summary(self.sess.run(self.Merge, feed_dict={\n",
    "                                    self.summary_reward1: r1, \n",
    "                                    self.summary_reward2: r2, \n",
    "                                    self.summary_reward3: r3}), episode)\n",
    "\n",
    "# Main 함수 -> DDPG 에이전트를 드론 환경에서 학습\n",
    "if __name__ == '__main__':\n",
    "    # 유니티 환경 설정\n",
    "    env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "    brain_name1 = env.brain_names[0]\n",
    "    brain_name2 = env.brain_names[1]\n",
    "    brain_name3 = env.brain_names[2]\n",
    "\n",
    "    brain1 = env.brains[brain_name1]\n",
    "    brain2 = env.brains[brain_name2]\n",
    "    brain3 = env.brains[brain_name3]\n",
    "\n",
    "    env_info = env.reset(train_mode=train_mode)\n",
    "\n",
    "    # DDPGAgnet 선언\n",
    "    agent1 = DDPGAgent(\"1\")\n",
    "    agent2 = DDPGAgent(\"2\")\n",
    "    agent3 = DDPGAgent(\"3\")\n",
    "\n",
    "    rewards1 = []\n",
    "    losses1 = []\n",
    "    rewards2 = []\n",
    "    losses2 = []\n",
    "    rewards3 = []\n",
    "    losses3 = []\n",
    "    step = 0\n",
    "\n",
    "    # 각 에피소드를 거치며 replay memory에 저장\n",
    "    for episode in range(run_episode + test_episode):\n",
    "        if episode == run_episode:\n",
    "            train_mode = False\n",
    "\n",
    "        env_info = env.reset(train_mode=train_mode)\n",
    "        done = False\n",
    "\n",
    "        state1 = env_info[brain_name1].vector_observations[0]\n",
    "        episode_reward1 = 0\n",
    "        done1 = False\n",
    "\n",
    "        state2 = env_info[brain_name2].vector_observations[0]\n",
    "        episode_reward2 = 0\n",
    "        done2 = False\n",
    "\n",
    "        state3 = env_info[brain_name3].vector_observations[0]\n",
    "        episode_reward3 = 0\n",
    "        done3 = False\n",
    "\n",
    "        while not done:\n",
    "            step += 1\n",
    "\n",
    "            action1 = agent1.get_action([state1])\n",
    "            action2 = agent2.get_action([state2])\n",
    "            action3 = agent3.get_action([state3])\n",
    "\n",
    "            env_info = env.step(vector_action={brain_name1:[action1], brain_name2:[action2], brain_name3:[action3]})\n",
    "\n",
    "            next_state1 = env_info[brain_name1].vector_observations[0]\n",
    "            reward1 = env_info[brain_name1].rewards[0]\n",
    "            episode_reward1 += reward1\n",
    "            done1 = env_info[brain_name1].local_done[0]\n",
    "\n",
    "            next_state2 = env_info[brain_name2].vector_observations[0]\n",
    "            reward2 = env_info[brain_name2].rewards[0]\n",
    "            episode_reward2 += reward2\n",
    "            done2 = env_info[brain_name2].local_done[0]\n",
    "\n",
    "            next_state3 = env_info[brain_name3].vector_observations[0]\n",
    "            reward3 = env_info[brain_name3].rewards[0]\n",
    "            episode_reward3 += reward3\n",
    "            done3 = env_info[brain_name3].local_done[0]\n",
    "\n",
    "            done = done1 and done2 and done3\n",
    "            \n",
    "            if train_mode:\n",
    "                agent1.append_sample(state1, action1[0], reward1, next_state1, done1)\n",
    "                agent2.append_sample(state2, action2[0], reward2, next_state2, done2)\n",
    "                agent3.append_sample(state3, action3[0], reward3, next_state3, done3)\n",
    "\n",
    "            state1 = next_state1\n",
    "            state2 = next_state2\n",
    "            state3 = next_state3\n",
    "\n",
    "            # train_mode 이고 일정 이상 에피소드가 지나면 학습\n",
    "            if episode > start_train_episode and train_mode :\n",
    "                agent1.train_model()\n",
    "                agent2.train_model()\n",
    "                agent3.train_model()\n",
    "\n",
    "        \n",
    "        rewards1.append(episode_reward1)\n",
    "        rewards2.append(episode_reward2)\n",
    "        rewards3.append(episode_reward3)\n",
    "\n",
    "        # 일정 이상의 episode를 진행 시 log 출력\n",
    "        if episode % print_interval == 0 and episode != 0:\n",
    "            print(\"step : {} / episode : {} / r1: {:.3f} / r2: {:.3f} / r3: {:.3f}\".format\n",
    "                  (step, episode, np.mean(rewards1), np.mean(rewards2), np.mean(rewards3)))\n",
    "            agent1.Write_Summray(np.mean(rewards1), np.mean(rewards2), np.mean(rewards3), episode)\n",
    "\n",
    "        rewards1 = []\n",
    "        losses1 = []\n",
    "        rewards2 = []\n",
    "        losses2 = []\n",
    "        rewards3 = []\n",
    "        losses3 = []\n",
    "        step = 0\n",
    "\n",
    "        # 일정 이상의 episode를 진행 시 현재 모델 저장\n",
    "        if train_mode and episode % save_interval == 0 and episode != 0:\n",
    "            print(\"model saved\")\n",
    "            agent1.save_model(str(episode))\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
